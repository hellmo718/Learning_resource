开篇废话：

很感谢谭哥的开篇废话这四个字，让我把一些废话说出来了，是时候还给谭哥了。因为废话太多会让人感觉，没有能力净废话。

今天我开始从头学习CNN，上一篇MachineLN之深度学习入门坑太多了，需要慢慢的填起来。那么我的问题是：

（1）为什么要提出cnn？

（2）cnn的结构是怎么样子的？

（3）cnn中各层的含义是什么？

看到这里你的答案是什么？ 那么下面是我的答案，欢迎批评指正。

**（1）为什么要提出cnn？**

嫌字太多直接看图，或者简单总结为cnn**参数少好学习**。

**卷积神经网络**是在神经网络的理论基础上形成的深度学习网络，它是一种特殊的多层神经网络。而传统的神经网络是一个全连接的网络结构（后面会提到），它上一层的每一个神经元与下一层的每一个神经元均有连接。这种结构有以下缺点：1、在处理声音和图像数据的时候，由于声音和图像的输入维度较高，包含数百个以上的变量，例如，输入图像的像素是100×100，假设隐含层要学习100维的特征 (即隐含层有 100 个神经元)，那么全连接网络就要学100×100×100个参数，即100万个权重参数，这样的网络结构在使用BP算法训练的时候，不但训练速度慢，而且需要的训练样本的数量也越多，若训练样本数量不足，会产生过拟合现象，学习得到的模型没有实用性。2、传统神经网络的结构对输入数据的特点的考虑不足，以图像识别为例，将同一幅图像做很小的位移，传统神经网络对其会很敏感，会当成是不同的图像，无法根据训练过程对该类数据特征进行优化处理。3、传统神经网络因为与输入数据是全连接的，无法识别训练数据中的局部区域特征，可是卷积神经网络可以单独学习识别该局部区域特征。

**==>看一下图（借用大神的图）吧： 主要看第一个（10的12次幂）和第四个（10k）图的参数数量相差10的8次幂倍，已经到了亿级别了。（local conv意思是每个卷积核是不同的，也就是后面提到的它不是权值共享）**

![image](http://upload-images.jianshu.io/upload_images/4618424-0496ff657fc51883?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

![image](http://upload-images.jianshu.io/upload_images/4618424-918179e78134f561?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**（2）cnn的结构是怎么样子的？**

下面是最经典的cnn结构，如果你关注cnn发展过程，是这样子的：LeNet5->AlexNet->VGG->Inception_v1(v2,v3)->resnet->Inception_v4->xception->resnet_v2->Inception_resnet等等，还有一些移动端的小模型（mobilenet、shuffleNet）。

![image](http://upload-images.jianshu.io/upload_images/4618424-d03660c647084f25?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

*   1\. 学过数字图像处理的应该学过卷积，像sobel算子等一些边缘检测，像一些均值滤波、中值滤波等等，只是这个卷积核我们称为fliters；但是和卷积神经网络中是不一样的，cnn中的卷积核往往是很多个，并且卷积中的卷积核值是通过学习得到的。**卷积的流程：**以一个很简单的图示展示卷积的流程：（下面只是一个简单的演示，真实情况下一般都是m个输入n个输出，对应[m,n,kernel_size,kernel_size]）

![image](http://upload-images.jianshu.io/upload_images/4618424-1ce00bdf4f836fa3?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

*   2\. 池化流程，在一个给定的区域内，譬如2*2的区域取最大值（最大池化）、平均值（平均池化），（还有随机池化等）然后设置步长一般为2（就是下一步3*3的区域走到哪里），这样遍历完后，图像大小则会变为原来的二分之一。下面是最大池化和平均池化。

![image](http://upload-images.jianshu.io/upload_images/4618424-c3f317322e84cfa4?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**（3）cnn中各层的含义是什么？**

**卷积层的作用：**

*   1\. 权值共享，减少训练参数；一个卷积核与 输入map的不同区域做卷积时，它的参数是固定不变的。在CNN里，这叫做权值更享，那么为什么说减少训练参数呢？ 没有对比不能说少了或者多了，在上面的为什么提出cnn中已经解释了。

*   2\. 不同的卷积核可以提取不同的特征；

**池化层的作用：**

减少参数。通过对卷积后的特征图降维，有效减少后续层需要的参数，但是下面的才是内涵所在：

*   1\. 使构建更深层次的网络变得可行；

*   2\. 使得filters获得更多的全局和contextual（上下文）信息；

*   3\. 使训练可行，也可以说使得训练变得更高效，主要是针对深层次的网络结构来说；

*   4\. 使得 特征map大小和数量进行更好的选择（权衡）。例如，就用输入到全连接层的前一层conv来说，特征map太大的话，特征数量就不易太多，通过pooling，使得特征map变小，特征map数量就可以更多。

    （那么为什么要特征map更多呢？好处在哪里？）

*   答：因为每个特征map对应一个filters，特征map越多对应更多的filters，而不同的filters提取的是图像中不同方面的特征，也就是说filters越多对图像不同特征的提取越多。

*   5. 还有局部旋转不变性哦，其中像素在邻域发生微小位移时，池化层的输出是不变的。这就使网络的鲁棒性增强了，有一定抗扰动的作用。

**最后层：全连结层（FC）**

这个简单提一下，水太深；

*   1. FC在整个卷积神经网络中起到“分类器”的作用；

*   2. 目前由于全连接层参数冗余（仅全连接层参数就可占整个网络参数80%左右），像ResNet和GoogLeNet等均用全局平均池化（GAP）取代FC来融合学到的深度特征，最后用softmax等损失函数作为网络目标函数训练模型。

*   3\. FC越来越不被看好。

说明：我只是根据自己的理解写了下来，我又不是大牛、大神，只是一个小罗罗，希望大家能给给予批评指正，另外挖的坑太多，欢迎投稿，救救我。

**展望：**

接下来请关注手撕cnn；

接下来请关注cnn实现；

接下来请关注：但之后dl将告一段落，补习传统机器学习的理论知识 到 实践；

后面再开始进入dl：搭建通用分类模型框架（vgg，resnet，inception等）；人脸检测系列；人脸识别系列；验证码识别系列；通用OCR系列；年龄性别识别；rnn预测；强化学习；一起走进无人驾驶；之间还会插入数据结构和算法；

目前自己在瓶颈期，真的掉坑里了，整理总结前行，一直在路上。

machinelp与你一年之约，你准备好了吗？

推荐阅读：

1. [MachineLN之三要素](http://mp.weixin.qq.com/s?__biz=MzU3MTM3MTIxOQ==&mid=2247483841&idx=2&sn=e4a3cff7b12c48af237c577c487ba3a1&chksm=fce07a7dcb97f36be5003c3018b3a391070bdc4e56839cb461d226113db4c5f24032e0bf5809&scene=21#wechat_redirect)

2. [MachineLN之模型评估](http://mp.weixin.qq.com/s?__biz=MzU3MTM3MTIxOQ==&mid=2247483872&idx=2&sn=8436e1eb9055d3a372278ee8688cd703&chksm=fce07a5ccb97f34a4490f60304b206c741d2395149c2c2e68bddb3faf7daf9121ca27a5d6a97&scene=21#wechat_redirect)

3. [MachinLN之dl](http://mp.weixin.qq.com/s?__biz=MzU3MTM3MTIxOQ==&mid=2247483894&idx=2&sn=63333c02674e15e84159e064073fe563&chksm=fce07a4acb97f35cc38f75dc891a19129e2406270d04b739cfa9b8a28f9780b4e2a65a7cd39b&scene=21#wechat_redirect)

![image.png](http://upload-images.jianshu.io/upload_images/4618424-12277859f8b95664.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

版权声明：本文为博主原创文章，未经博主允许不得转载。有问题可以加微信：lp9628(注明CSDN)。
