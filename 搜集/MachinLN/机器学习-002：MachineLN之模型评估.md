

开篇废话：

很多文章其实都是将书中的东西、网上课程、或者别人的论文的东西总结一下，发出来，但是个人感觉还是加入个人的理解，然后加上一些工程中遇到的问题一起发出来会更好些。

正如学习再多tricks，不如踩一遍坑。 模型评估的标准有很多，但是适合自己的才是好的，我本人就比较喜欢看loss，而有些人更加关注准确率。

回想上学时候我们基本接受的都是被动接收知识，其实教的再好都不如好的提问，学习独立思考的能力，我学习的很多东西不是在课堂上老师所谓的教，而是在面试过程中面试官的提问，在项目中老板的提问、客户的提问，每次我都能学到很多东西，因为他们问的我很多有点含糊，或者不会，然后就回来自己学习总结，接下来又去被问，又学习又总结，也因此刚毕业换了几次工作，看到这里大家肯定感觉我是奇葩中奇葩，其实我不反对，别人怎么看我并不重要，只要自己知道自己想要的是什么就好，有人可能会问，刚毕业就换工作是不是不太好？ 我可以跟你说没关系，迈开第一步，万事大吉。

那么我的问题是：

（1）为什么要评估模型？

（2）评估模型有哪些方法？

（3）不同的方法针对什么问题？

（4）根据评估的结果如何调优？

（5）根据评估结果怎么判定模型训练完成？

看到这里你的答案是什么？ 那么下面是我的答案，欢迎批评指正。

（1）为什么要评估模型？

其实我们去评估模型最终是为了，得到符合我们数据或者是业务的最优模型，但是这往往不是一蹴而就的，反而使得评估模型通常成了下一步我们调参或者调优的一个参考。

（2）评估模型有哪些方法？

通常很多书籍中不会将loss（MachineLN之三要素中策略，也就是你的损失函数、目标函数的值）作为模型的评估标准，反而loss是一个很重要的标准，他将左右你调参过程中的很多参数，并且可以成为你判定是否模型训练完成的标准，譬如我们看到loss一直波动比较大，我们很自然的会想到是你的学习率太大了吧，后来loss开始波动很大，回来慢慢的平稳下降，这可能是数据分布的影响，可能是你样本的预处理方式；个人喜欢看loss，而准确率有时候在很长时间里波动不会很大；看到这里不要迷糊，总之还是那句话自己去踩踩坑，比什么都重要。

模型评估的方法相比大家都不陌生，只要看过一些机器学习的基础知识都可以知道一些，我在想要不要再写一下呢，纠结中...

评价的方法有：

错误率(error rate)= a个样本分类错误/m个样本精度(accuracy)= 1 -错误率

误差(error)：学习器实际预测输出与样本的真是输出之间的差异（差异的标准可以定义为相似度或者距离）。

训练误差(training error)：即经验误差。学习器在训练集上的误差。

泛化误差(generalization error)：学习器在新样本上的误差。

真正例（True Positive，TP）：真实类别为正例，预测类别为正例。

假正例（False Positive，FP）：真实类别为负例，预测类别为正例。

假负例（False Negative，FN）：真实类别为正例，预测类别为负例。

真负例（True Negative，TN）：真实类别为负例，预测类别为负例。

![image](http://upload-images.jianshu.io/upload_images/4618424-6286432751d9c83a?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**ROC：** 纵轴：真正例率 TPR；横轴：假正例率FPR；

![image](http://upload-images.jianshu.io/upload_images/4618424-b4119ada9be31a13?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**AUC：**ROC曲线下面的面积。

**准确率，又称查准率（Precision，P）；**

![image](http://upload-images.jianshu.io/upload_images/4618424-9805847e7a3deecc?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

**召回率，又称查全率（Recall，R）：（例如在人脸检测中有多少人脸检测到了，漏掉了多少人脸）**

![image](http://upload-images.jianshu.io/upload_images/4618424-61aa9658db9936b4?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

等；

上面基本上是分类模型的评价标准，而在回归模型中，一般都是通过距离或者相似度来衡量（欧氏距离等）。有点乱，可以去看看周老师的机器学习一书。

（3）不同的方法针对什么问题？

在（2）中回答过了。

（4）根据评估的结果如何调优？（都是基于深度学习的，?）

*   当训练集的效果（准确率）上不去，和贝叶斯估计（人的表现）存在一定差距的时候：（1）增加模型的复杂度。 或者直接多加几层。（2）现在目前效果非常好的一些模型：resnet，inception等。（3）重新选择优化函数：AdamOptimizer，MomentumOptimizer，RMSPropOptimizer等等。

*   训练集效果表现好，测试集效果表现不好：（1）增加训练样本的数量。（2）正则化：L2范数，dropout等（dropout原理是什么呢？使随机使神经元失活，相当于把一个复杂的模型拆分开，测试后时候凑到一起，集成学习的思想，又刹不住闸了。。。）。（3）还要观察训练样本和测试样本的分布是否一致。（4）交叉验证。

*   还有像样本预处理：（1）归一化：img/255.0，img-0.5, img*2，将数据转化为[-1,1].（2）减去均值除以方差。（3）减去样本各通道的均值。

*   还有loss的选择啊，多标签单分类适合用softmax_cross_entropy（对于多标签多分类的要用多个softmax），多标签多分类常用sigmoid_cross_entropy。

*   data augmentation。

*   还要注意BN的使用，学习率的选择，batch_size的大小。

可以参考博客：http://blog.csdn.net/u014365862/article/details/78519727

深度学习的这么坑你都遇到过吗？：http://blog.csdn.net/u014365862/article/details/77961624

（5）根据评估结果怎么判定模型训练完成？

很多人都说：loss不再怎么变化，或者准确率不再怎么变化，ML书中一般都是交叉验证选最好的，但是dl中往往是选择什么时候停止，其实真实的情况往往是这个样子的（如下图），开始模型是欠拟合的，随着迭代次数的增多，模型会慢慢收敛，但是过了一个点以后，会呈现过拟合，这种情况怎么调参？ 有哪些解决方法？ 哦，发现自己好烦，我同事和大家一样也很烦我，说我问题真多，哈哈。。。我在迁移学习过程中一般都是从更新最后一层参数开始，根据自己的数据量来判断要更新最后几层的参数，更新多了会过拟合，少了会欠拟合，当然你还可以设置正则化等等。

![image.png](http://upload-images.jianshu.io/upload_images/4618424-04ea97d9e780d070.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)


（6）总结

模型评估还是很重要的，但注意不同的标准可能评估的结果有点差异，还想简单提一下，偏差和方差的问题，后面的学习中可能会用到，先看一下这张图（其实就是上面那张图），其实我们模型的误差Error = Bias + Variance + Noise。 鱼和熊掌不可兼得。

可以参考：http://blog.csdn.net/u014365862/article/details/76360351。

![image](http://upload-images.jianshu.io/upload_images/4618424-352788ab73ed0d7f?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240) 

![image](http://upload-images.jianshu.io/upload_images/4618424-1e979c54b29c1552?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

</article>

版权声明：本文为博主原创文章，未经博主允许不得转载。有问题可以加微信：lp9628(注明CSDN)。
